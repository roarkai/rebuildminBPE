{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_tokenizer import BasicTokenizer\n",
    "from regex_tokenizer import RegexTokenizer\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1. test the function of BasicTokenizer and RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "with open('toy.txt', 'r') as f:\n",
    "    toy_text = f.read()\n",
    "\n",
    "print(len(toy_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters:\n",
    "vocab_size = 256 + 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model and train a BasicTokenizer\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "basic_tokenizer.train(toy_text, vocab_size)\n",
    "# for p, idx in tokenizer.merges.items():\n",
    "#     print(f'{p} => {idx}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come on rk\n"
     ]
    }
   ],
   "source": [
    "# simple test the function of BasicTokenizer\n",
    "encode = basic_tokenizer.encode\n",
    "decode = basic_tokenizer.decode\n",
    "print(decode(encode(\"come on rk\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model and train a RegexTokenizer\n",
    "test_text = toy_text\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re_tokenizer = RegexTokenizer(GPT4_SPLIT_PATTERN)\n",
    "re_tokenizer.train(test_text, vocab_size, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello word! this is hello from rk'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple test the function of RegexTokenizer\n",
    "re_encode = re_tokenizer.encode\n",
    "re_decode = re_tokenizer.decode\n",
    "re_decode(re_encode('hello word! this is hello from rk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2. compare RegexTokenizer and GPT4 tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'hello world!!!? (안녕하세요!) lol123 😉'\n",
    "enc_gpt4 = tiktoken.get_encoding('cl100k_base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_basic = basic_tokenizer.encode(text)\n",
    "ids_regex = re_tokenizer.encode(text)\n",
    "ids_gpt4 = enc_gpt4.encode(text)\n",
    "\n",
    "text_basic = basic_tokenizer.decode(ids_basic)\n",
    "text_gpt4 = enc_gpt4.decode(ids_gpt4)\n",
    "text_regex = re_tokenizer.decode(ids_regex)\n",
    "text_gpt4 == text_regex == text_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part3. test the rebuild GPT4 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function used in recover_merges()\n",
    "def bpe(mergable_ranks, token, max_rank):\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = min_rank = None\n",
    "        # print(parts)\n",
    "        # find the pair that has the lowest rank\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "        \n",
    "        # if len(parts) == 2, stop loop\n",
    "        if min_rank == max_rank:\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        # otherwise, continue the merge loop\n",
    "        parts = parts[:min_idx] + \\\n",
    "                [parts[min_idx] + parts[min_idx + 1]] + \\\n",
    "                parts[min_idx + 2:]\n",
    "    \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the dictionary of merges\n",
    "def recover_merges(mergeable_ranks):\n",
    "    # the `merges` are already the byte sequences in their merged state.\n",
    "    # so we have to recover the original pairings. We can do this by doing\n",
    "    # a small BPE training run on all the tokens, in their order.\n",
    "    merges = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        if len(token) == 1:\n",
    "            continue # skip raw bytes\n",
    "        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "        assert len(pair) == 2\n",
    "        \n",
    "        # recover the integer ranks of the pair\n",
    "        ix0 = mergeable_ranks[pair[0]]\n",
    "        ix1 = mergeable_ranks[pair[1]]\n",
    "        merges[(ix0, ix1)] = rank\n",
    "\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    bs = [b for b in range(256) if chr(b).isprintable() and chr(b) != ' ']\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def build_shuffled_vocab(merges):\n",
    "    vocab = {}\n",
    "    byte_unicode = bytes_to_unicode()\n",
    "    for i, bi in enumerate(byte_unicode):\n",
    "        vocab[i] = bytes([bi])\n",
    "    for (p0, p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "mergeable_ranks = enc._mergeable_ranks\n",
    "merges = recover_merges(mergeable_ranks)\n",
    "vocab1 = build_shuffled_vocab(merges)\n",
    "vocab2 = {v: k for k, v in mergeable_ranks.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100256 100256\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab1), len(vocab2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100256\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in range(100256):\n",
    "    if vocab1[i] == vocab2[i]:\n",
    "        x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'!'\n",
      "1 b'\"'\n",
      "2 b'#'\n",
      "3 b'$'\n",
      "4 b'%'\n",
      "5 b'&'\n",
      "6 b\"'\"\n",
      "7 b'('\n",
      "8 b')'\n",
      "9 b'*'\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k, v in vocab1.items():\n",
    "    if i < 10:\n",
    "        print(k,  v)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## view the contents in the dictionary '_mergeable_ranks' of gpt4\n",
    "\n",
    "# with open('gpt4_mergeable_ranks.txt', 'w') as f:\n",
    "#     for i, (k, v) in enumerate(mergeable_ranks.items()):\n",
    "#         if (i + 1) % 8 == 0:\n",
    "#             f.write(f'{k.ljust(16)}: {v:6d}\\n')\n",
    "#         else:\n",
    "#             f.write(f'{k.ljust(16)}: {v:6d} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'!':   0| b'\"':   1| b'#':   2| b'$':   3| b'%':   4| b'&':   5| b\"'\":   6| b'(':   7| b')':   8| b'*':   9| b'+':  10| b',':  11\n",
      "b'-':  12| b'.':  13| b'/':  14| b'0':  15| b'1':  16| b'2':  17| b'3':  18| b'4':  19| b'5':  20| b'6':  21| b'7':  22| b'8':  23\n",
      "b'9':  24| b':':  25| b';':  26| b'<':  27| b'=':  28| b'>':  29| b'?':  30| b'@':  31| b'A':  32| b'B':  33| b'C':  34| b'D':  35\n",
      "b'E':  36| b'F':  37| b'G':  38| b'H':  39| b'I':  40| b'J':  41| b'K':  42| b'L':  43| b'M':  44| b'N':  45| b'O':  46| b'P':  47\n",
      "b'Q':  48| b'R':  49| b'S':  50| b'T':  51| b'U':  52| b'V':  53| b'W':  54| b'X':  55| b'Y':  56| b'Z':  57| b'[':  58| b'\\\\':  59\n",
      "b']':  60| b'^':  61| b'_':  62| b'`':  63| b'a':  64| b'b':  65| b'c':  66| b'd':  67| b'e':  68| b'f':  69| b'g':  70| b'h':  71\n",
      "b'i':  72| b'j':  73| b'k':  74| b'l':  75| b'm':  76| b'n':  77| b'o':  78| b'p':  79| b'q':  80| b'r':  81| b's':  82| b't':  83\n",
      "b'u':  84| b'v':  85| b'w':  86| b'x':  87| b'y':  88| b'z':  89| b'{':  90| b'|':  91| b'}':  92| b'~':  93| b'\\xa1':  94| b'\\xa2':  95\n",
      "b'\\xa3':  96| b'\\xa4':  97| b'\\xa5':  98| b'\\xa6':  99| b'\\xa7': 100| b'\\xa8': 101| b'\\xa9': 102| b'\\xaa': 103| b'\\xab': 104| b'\\xac': 105| b'\\xae': 106| b'\\xaf': 107\n",
      "b'\\xb0': 108| b'\\xb1': 109| b'\\xb2': 110| b'\\xb3': 111| b'\\xb4': 112| b'\\xb5': 113| b'\\xb6': 114| b'\\xb7': 115| b'\\xb8': 116| b'\\xb9': 117| b'\\xba': 118| b'\\xbb': 119\n",
      "b'\\xbc': 120| b'\\xbd': 121| b'\\xbe': 122| b'\\xbf': 123| b'\\xc0': 124| b'\\xc1': 125| b'\\xc2': 126| b'\\xc3': 127| b'\\xc4': 128| b'\\xc5': 129| b'\\xc6': 130| b'\\xc7': 131\n",
      "b'\\xc8': 132| b'\\xc9': 133| b'\\xca': 134| b'\\xcb': 135| b'\\xcc': 136| b'\\xcd': 137| b'\\xce': 138| b'\\xcf': 139| b'\\xd0': 140| b'\\xd1': 141| b'\\xd2': 142| b'\\xd3': 143\n",
      "b'\\xd4': 144| b'\\xd5': 145| b'\\xd6': 146| b'\\xd7': 147| b'\\xd8': 148| b'\\xd9': 149| b'\\xda': 150| b'\\xdb': 151| b'\\xdc': 152| b'\\xdd': 153| b'\\xde': 154| b'\\xdf': 155\n",
      "b'\\xe0': 156| b'\\xe1': 157| b'\\xe2': 158| b'\\xe3': 159| b'\\xe4': 160| b'\\xe5': 161| b'\\xe6': 162| b'\\xe7': 163| b'\\xe8': 164| b'\\xe9': 165| b'\\xea': 166| b'\\xeb': 167\n",
      "b'\\xec': 168| b'\\xed': 169| b'\\xee': 170| b'\\xef': 171| b'\\xf0': 172| b'\\xf1': 173| b'\\xf2': 174| b'\\xf3': 175| b'\\xf4': 176| b'\\xf5': 177| b'\\xf6': 178| b'\\xf7': 179\n",
      "b'\\xf8': 180| b'\\xf9': 181| b'\\xfa': 182| b'\\xfb': 183| b'\\xfc': 184| b'\\xfd': 185| b'\\xfe': 186| b'\\xff': 187| b'\\x00': 188| b'\\x01': 189| b'\\x02': 190| b'\\x03': 191\n",
      "b'\\x04': 192| b'\\x05': 193| b'\\x06': 194| b'\\x07': 195| b'\\x08': 196| b'\\t': 197| b'\\n': 198| b'\\x0b': 199| b'\\x0c': 200| b'\\r': 201| b'\\x0e': 202| b'\\x0f': 203\n",
      "b'\\x10': 204| b'\\x11': 205| b'\\x12': 206| b'\\x13': 207| b'\\x14': 208| b'\\x15': 209| b'\\x16': 210| b'\\x17': 211| b'\\x18': 212| b'\\x19': 213| b'\\x1a': 214| b'\\x1b': 215\n",
      "b'\\x1c': 216| b'\\x1d': 217| b'\\x1e': 218| b'\\x1f': 219| b' ': 220| b'\\x7f': 221| b'\\x80': 222| b'\\x81': 223| b'\\x82': 224| b'\\x83': 225| b'\\x84': 226| b'\\x85': 227\n",
      "b'\\x86': 228| b'\\x87': 229| b'\\x88': 230| b'\\x89': 231| b'\\x8a': 232| b'\\x8b': 233| b'\\x8c': 234| b'\\x8d': 235| b'\\x8e': 236| b'\\x8f': 237| b'\\x90': 238| b'\\x91': 239\n",
      "b'\\x92': 240| b'\\x93': 241| b'\\x94': 242| b'\\x95': 243| b'\\x96': 244| b'\\x97': 245| b'\\x98': 246| b'\\x99': 247| b'\\x9a': 248| b'\\x9b': 249| b'\\x9c': 250| b'\\x9d': 251\n",
      "b'\\x9e': 252| b'\\x9f': 253| b'\\xa0': 254| b'\\xad': 255| "
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(mergeable_ranks.items()):\n",
    "    if (i + 1) % 12 == 0:\n",
    "        print(f'{k}: {v:3d}')\n",
    "    elif i >= 256:\n",
    "        break\n",
    "    else:\n",
    "        print(f'{k}: {v:3d}', end = '| ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33: ! 34: \" 35: # 36: $ 37: % 38: & 39: ' 40: ( 41: ) 42: * 43: + 44: ,\n",
      "45: - 46: . 47: / 48: 0 49: 1 50: 2 51: 3 52: 4 53: 5 54: 6 55: 7 56: 8\n",
      "57: 9 58: : 59: ; 60: < 61: = 62: > 63: ? 64: @ 65: A 66: B 67: C 68: D\n",
      "69: E 70: F 71: G 72: H 73: I 74: J 75: K 76: L 77: M 78: N 79: O 80: P\n",
      "81: Q 82: R 83: S 84: T 85: U 86: V 87: W 88: X 89: Y 90: Z 91: [ 92: \\\n",
      "93: ] 94: ^ 95: _ 96: ` 97: a 98: b 99: c 100: d 101: e 102: f 103: g 104: h\n",
      "105: i 106: j 107: k 108: l 109: m 110: n 111: o 112: p 113: q 114: r 115: s 116: t\n",
      "117: u 118: v 119: w 120: x 121: y 122: z 123: { 124: | 125: } 126: ~ 161: ¡ 162: ¢\n",
      "163: £ 164: ¤ 165: ¥ 166: ¦ 167: § 168: ¨ 169: © 170: ª 171: « 172: ¬ 174: ® 175: ¯\n",
      "176: ° 177: ± 178: ² 179: ³ 180: ´ 181: µ 182: ¶ 183: · 184: ¸ 185: ¹ 186: º 187: »\n",
      "188: ¼ 189: ½ 190: ¾ 191: ¿ 192: À 193: Á 194: Â 195: Ã 196: Ä 197: Å 198: Æ 199: Ç\n",
      "200: È 201: É 202: Ê 203: Ë 204: Ì 205: Í 206: Î 207: Ï 208: Ð 209: Ñ 210: Ò 211: Ó\n",
      "212: Ô 213: Õ 214: Ö 215: × 216: Ø 217: Ù 218: Ú 219: Û 220: Ü 221: Ý 222: Þ 223: ß\n",
      "224: à 225: á 226: â 227: ã 228: ä 229: å 230: æ 231: ç 232: è 233: é 234: ê 235: ë\n",
      "236: ì 237: í 238: î 239: ï 240: ð 241: ñ 242: ò 243: ó 244: ô 245: õ 246: ö 247: ÷\n",
      "248: ø 249: ù 250: ú 251: û 252: ü 253: ý 254: þ 255: ÿ 0: Ā 1: ā 2: Ă 3: ă\n",
      "4: Ą 5: ą 6: Ć 7: ć 8: Ĉ 9: ĉ 10: Ċ 11: ċ 12: Č 13: č 14: Ď 15: ď\n",
      "16: Đ 17: đ 18: Ē 19: ē 20: Ĕ 21: ĕ 22: Ė 23: ė 24: Ę 25: ę 26: Ě 27: ě\n",
      "28: Ĝ 29: ĝ 30: Ğ 31: ğ 32: Ġ 127: ġ 128: Ģ 129: ģ 130: Ĥ 131: ĥ 132: Ħ 133: ħ\n",
      "134: Ĩ 135: ĩ 136: Ī 137: ī 138: Ĭ 139: ĭ 140: Į 141: į 142: İ 143: ı 144: Ĳ 145: ĳ\n",
      "146: Ĵ 147: ĵ 148: Ķ 149: ķ 150: ĸ 151: Ĺ 152: ĺ 153: Ļ 154: ļ 155: Ľ 156: ľ 157: Ŀ\n",
      "158: ŀ 159: Ł 160: ł 173: Ń "
     ]
    }
   ],
   "source": [
    "# {int: str}\n",
    "byte_code = bytes_to_unicode()\n",
    "print(byte_code)\n",
    "for i, (byte, code) in enumerate(byte_code.items()):\n",
    "    if (i + 1) % 12 == 0:\n",
    "        print(f'{byte}: {code}')\n",
    "    else:\n",
    "        print(f'{byte}: {code}', end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
